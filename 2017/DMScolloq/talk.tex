\documentclass[hide notes,intlimits,usenames,dvipsnames]{beamer}

\mode<presentation>
{
  \usetheme{Singapore}
  \usefonttheme{professionalfonts}
  \setbeamertemplate{blocks}[rounded][shadow=true]
  \setbeamercovered{transparent}
  \setbeamertemplate{footline}[frame number]
}

% load packages
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[multidot]{grffile}
\usepackage{amsmath,verbatim,empheq,xspace}

\usepackage{tikz}
\usetikzlibrary{shapes,shadows,fadings}
\usetikzlibrary{decorations.markings,decorations.pathreplacing}
\usetikzlibrary{arrows,arrows.meta}

%\usepackage{animate}

\usepackage[noend]{algpseudocode}
\usepackage{listings}

% see http://tex.stackexchange.com/questions/86188/labelling-with-arrows-in-an-automated-way

\newif\ifclipme\clipmetrue
\tikzset{labelstyle/.style={LabelStyle/.append style={#1}},linestyle/.style={LineStyle/.append style={#1}}}
\tikzset{LabelStyle/.initial={},LineStyle/.initial={}}

\newcommand{\mathWithDescription}[4][]{{%
    \tikzset{#1}%
    \tikz[baseline]{
        \node[draw=red,rounded corners,anchor=base] (m#4) {$\displaystyle#2$};
        \ifclipme\begin{pgfinterruptboundingbox}\fi
            \node[above of=m#4,font=\strut, LabelStyle] (l#4) {#3};
            \draw[-,red, LineStyle] (l#4) to (m#4);
        \ifclipme\end{pgfinterruptboundingbox}\fi
    }%
}}

\newcommand{\mathWithDescriptionStarred}[3][]{{%
    \clipmefalse%
    \mathWithDescription[#1]{#2}{#3}{\themathLabelNode}%
}}

\newcounter{mathLabelNode}

\newcommand{\mathLabelBox}[3][]{%
   \stepcounter{mathLabelNode}%
   \mathWithDescription[#1]{#2}{#3}{\themathLabelNode}%
   \vphantom{\mathWithDescriptionStarred[#1]{#2}{#3}{\themathLabelNode}}%
}

\definecolor{dark red}{HTML}{E41A1C}
\definecolor{dark green}{HTML}{4DAF4A}
\definecolor{dark violet}{HTML}{984EA3}
\definecolor{dark blue}{HTML}{084594}
\definecolor{dark orange}{HTML}{FF7F00}
\definecolor{light blue}{HTML}{377EB8}
\definecolor{light red}{HTML}{FB9A99}
\definecolor{light violet}{HTML}{CAB2D6}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Xcal}{\mathcal{X}}

\newcommand{\bF}{\mathbf{F}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bX}{\mathbf{X}}

\newcommand{\bb}{\mathbf{b}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

\newcommand{\Div}{\nabla\cdot}
\newcommand{\eps}{\epsilon}
\newcommand{\grad}{\nabla}
\newcommand{\lap}{\triangle}
\renewcommand{\bar}{\overline}

\newcommand{\ip}[2]{\ensuremath{\left<#1,#2\right>}}


\newcommand{\FM}{F$\begin{smallmatrix} \text{D} \\ \text{E} \end{smallmatrix}$M\xspace}


\newenvironment{transbox}[1][]{%
\begin{tikzpicture}
\node[drop shadow,rounded corners,text width=\textwidth,fill=white, fill opacity=#1,text opacity=1] \bgroup
}{
\egroup;\end{tikzpicture}}


\title{Optimal solvers for partial differential equations}

\author[Bueler]{Ed Bueler}

\institute[UAF]{
  \scriptsize Dept of Mathematics and Statistics and Geophysical Institute \\

  University of Alaska Fairbanks
}

%\titlegraphic{\includegraphics[width=\textwidth]{andycoast.png}}

\beamertemplatenavigationsymbolsempty   % remove faint and silly navigation symbols at bottom
\renewcommand{\insertnavigation}[1]{}   % remove section headings from top of each slide

\setbeamerfont{date}{size=\scriptsize}
\date{}

\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{Outline}
    %\tableofcontents[currentsection,hideallsubsections]
    \tableofcontents[currentsection]
  \end{frame}
}


\begin{document}

%\graphicspath{{../../old/commonfigs/}}

\begin{frame}
    \vspace{10mm}
    \titlepage
    \begin{center}
    \tiny DMS Colloquium \quad 28 November, 2017
    \end{center}
\end{frame}

%\begin{frame}
%    \frametitle{Outline}
%    \tableofcontents
%\end{frame}

\begin{frame}{why this talk?}

\begin{itemize}
\item I have been thinking about what is the \emph{goal} when numerically solving differential equations
\item there are reliable black boxes for ODE IVPs
	\begin{itemize}
	\item[$\circ$] \texttt{ode45} in \textsc{Matlab}
	\end{itemize}
\item how would you build a good PDE black box?
\end{itemize}
\end{frame}


\section{how to approximately solve a PDE}

\begin{frame}{Poisson equation}

\begin{itemize}
\item for much of this talk I'll use two example PDE problems

\bigskip
\item[\textbf{1.}] \emph{Poisson equation} with Dirichlet boundary conditions:
	    $$- \grad^2 u = f \qquad \text{ on } \Omega \subset \RR^d \text{ with } u\big|_{\partial \Omega} = g,$$
    \vspace{-5mm}
	\begin{itemize}
	\item[$\circ$] a linear elliptic PDE problem in dimension $d=2$ or $d=3$
	\item[$\circ$] recall that $\grad^2 u = \Div \left(\grad u\right) = u_{xx}+u_{yy}+u_{zz}$
	\item[$\circ$] source $f(x,y,z)$ given
	\item[$\circ$] boundary values $g(x,y,z)$ given
	\item[$\circ$] will use various domains $\Omega$ including a square, a cube, and
		\begin{itemize}
        \item a snowflake fractal \hspace{0.3in} \begin{tikzpicture}[scale=0.8,baseline] \input{tikz/snowflake.tex} \end{tikzpicture}
        \end{itemize}
	\item[$\circ$] the solution $u(x,y,z)$ of $- \grad^2 u = 1$ with $g=0$ gives the expected time for a Brownian motion to first hit $\partial\Omega$
	\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{minimal surface equation}

\begin{itemize}
\item[\textbf{2.}] \emph{minimal surface equation (MSE)} with Dirichlet b.c.s:
	    $$- \grad\cdot \left(\frac{\grad u}{\sqrt{1 + |\grad u|^2}}\right) = 0  \qquad \text{ with } u\big|_{\partial \Omega} = g.$$
    \vspace{-2mm}
	\begin{itemize}
	\item[$\circ$] a nonlinear elliptic PDE in 2D
	\item[$\circ$] here: square domain $\Omega = [0,1] \times [0,1]$
	\item[$\circ$] the solution $u(x,y)$ gives the height of a zero-gravity soap bubble which spans a wire frame with height $z=g(x,y)$:
	\end{itemize}

\begin{center}
\begin{tikzpicture}[scale=2.0] \input{tikz/tent.tex} \end{tikzpicture}
\end{center}
\end{itemize}
\end{frame}


\begin{frame}{today's talk: mostly elliptic PDEs}

both examples {\color{Blue} \textbf{1}} \& {\color{Blue} \textbf{2}}
\begin{itemize}
\item are well-posed elliptic PDE BVPs
\item seek solution $u$ from an \alert{$\infty$-dimensional vector space}
\item \emph{main idea}: a PDE BVP is a system of $\infty$ eqns in $\infty$ unknowns
\end{itemize}

\bigskip\bigskip\bigskip

\scriptsize
fine print:
\begin{itemize}
\item both examples derivable from variational principles, thus well-posed
\item the ``$\infty$-dimensional vector space'' is a Sobolev space such as $H^1(\Omega)$
%\item \emph{numerically}, nonlinear elliptic PDE BVPs are representative of the hardest class of PDE problems because they arise as the time-step problems in the (harder) stiff time-dependent case
%\item \dots but smoothness of the solution matters \emph{a lot} to numerical difficulty
\end{itemize}
\end{frame}


\begin{frame}{approximation: finite difference method}
\begin{itemize}
\item most problems are not solvable exactly, so we
	\begin{itemize}
	\item[$\circ$] \alert{approximate by $N$ equations in $N$ unknowns}
	\item[$\circ$] where $N\in\ZZ^+$ so $N \ll \infty$
	\end{itemize}
\item one method is \emph{finite differences} (FDM), based on
	    $$f'(x) = \lim_{h \to 0} \frac{f(x+h)-f(x)}{h} \approx \frac{f(x+h)-f(x)}{h}$$
\item as used for the 2D Poisson equation:
	    $$u_{xx}+u_{yy} \approx \frac{u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4 u_{ij}}{h^2}$$
\end{itemize}
\end{frame}


\begin{frame}{structured grids}
\begin{itemize}
\item for a much of this talk I'll use \emph{structured grids}
	\begin{itemize}
	\item[$\circ$] i.e.~products of grids in 1D
	\end{itemize}
\item sequence of such grids $\Omega^{(k)}$; the \emph{level} $k$ grid has
	\begin{itemize}
	\item[$\circ$] equal spacing $h_k$ in each direction
	\end{itemize}

\bigskip
\begin{tikzpicture}[scale=1.6]
\input{tikz/fourlevelsofgrids.tex}
\end{tikzpicture}

\medskip
\item $N_k$ is the number of equations and of unknowns
\item ``increasing resolution'' means $h_k \to 0$ and $N_k \to \infty$
	\begin{itemize}
	\item[$\circ$] a.k.a.~``refinement''
    \item[$\circ$] $N_k = O(h_k^{-d})$ grid points in $d$ dimensions
	\item[$\circ$] typically: $N_{k+1} = 2^d N_k$
	\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{approximation: finite element method (FEM)}
\begin{itemize}
\item this discretization uses the \emph{weak form} of the PDE
\item example: for the Poisson equation it is
    $$\int_\Omega \grad u \cdot \grad v = \int_\Omega f v \qquad \forall v \in H_0^1(\Omega)$$
    \vspace{-4mm}
	\begin{itemize}
	\item[$\circ$] derivation: multiply PDE by $v$ and integrate by parts
	\end{itemize}
\item FEM is well-suited to unstructured meshes of arbitrary polygonal/polyhedral domains
	\begin{itemize}
	\item[$\circ$] e.g.~triangulate the snowflake:
	\end{itemize}

\begin{center}
\begin{tikzpicture}[scale=1.3,baseline] \input{tikz/snowflake.tex} \end{tikzpicture}
\qquad $\to$ \qquad
\begin{tikzpicture}[scale=1.3,baseline] \input{tikz/meshedsnowflake.tex} \end{tikzpicture}
\end{center}

\footnotesize
\item \alert{Fall 2018: graduate seminar in FEM}
\end{itemize}
\end{frame}


\begin{frame}{finite element method}

in more detail, the FEM uses
\begin{itemize}
\item a {\footnotesize triangular/quadrilateral/tetrahedral/etc.}~mesh of $N$ nodes on $\Omega$
\item an $N$-dimensional subspace $\mathcal{X} \subset H^1(\Omega)$
\item \begin{minipage}[t]{55mm}
basis of \emph{hat functions} $\psi_j(x,y)$
	\begin{itemize}
	\item[$\circ$] $\psi_j(x_i,y_i)=0$ if $i\ne j$
	\item[$\circ$] $\psi_j(x_j,y_j)=1$
	\end{itemize}
\end{minipage}
\quad
\begin{minipage}[t]{35mm}
\vspace{-4mm}
\begin{tikzpicture}[scale=0.6, z={(.707,.3)}, baseline]
\input{tikz/hatfunction.tex}
\end{tikzpicture}
\end{minipage}
\end{itemize}

\vspace{-3mm}
then the FEM
\begin{itemize}
\item defines
   $$u(x,y) = \sum_j u_{j}\, \psi_{j}(x,y)$$
for unknown coefficients $\bu=\{u_j\}\in \RR^N$ ($N$ unknowns)
\item requires the weak form to hold for all $v=\psi_i$ ($N$ eqns)
\end{itemize}
\end{frame}


\begin{frame}{sparse matrices}
\begin{itemize}
\item both methods (\FM) produce \emph{sparse matrices}
\item for example, the Poisson equation becomes a linear system
    $$A \bu = \bb$$
    \vspace{-4mm}
	\begin{itemize}
	\item[$\circ$] $A\in\RR^{N\times N}$ is sparse
	\item[$\circ$] $A$ is symmetric positive definite (SPD)
	\end{itemize}
\item \emph{pro tip}: Matlab's \texttt{spy(A)} shows nonzero structure
\end{itemize}

\bigskip
\begin{center}
\includegraphics[width=0.85\textwidth]{figs/spythree}

\medskip
\scriptsize
Poisson 1D (either method) \quad MSE FDM 2D square \qquad Poisson FEM 2D snowflake

\end{center}
\end{frame}


\begin{frame}{nonlinear PDEs make sparse matrices too}
\begin{itemize}
\item \FM applied to a nonlinear elliptic PDE BVP gives a nonlinear (algebraic) system of $N$ equations in $N$ unknowns:
    $$\mathbf{F}(\bu)=0$$
     \vspace{-4mm}
	\begin{itemize}
	\item[$\circ$] $\mathbf{F}:\RR^N\to\RR^N$ is a nonlinear function
	\item[$\circ$] call $\mathbf{F}(\bw)$ the \emph{residual} if $\bw$ is a guess at the solution
	\end{itemize}

\item \begin{minipage}[t]{70mm}
usually apply Newton's method to solve:
\begin{align*}
J_{\mathbf{F}}(\bu_\ell)\, \bs &= - \mathbf{F}(\bu_\ell)  \\
    \bu_{\ell+1} &= \bu_\ell + \bs
\end{align*}
    \vspace{-6mm}
	\begin{itemize}
	\item[$\circ$] $J_{\mathbf{F}}(\bw) \in \RR^{N\times N}$ is the Jacobian of $\mathbf{F}$
	\item[$\circ$] it is a sparse matrix (right)
	\end{itemize}
\end{minipage}
\quad
\begin{minipage}[t]{20mm}
\vspace{0mm}
\hfill \includegraphics[width=0.6\textwidth]{figs/newton}

\includegraphics[width=\textwidth]{figs/spybanded}

\tiny
 MSE FDM 2D square
\end{minipage}
\end{itemize}
\end{frame}


\section{what is an ``optimal solver''?}

\begin{frame}{define ``optimal''}
\begin{itemize}
\item consider $N$ equations in $N$ unknowns: \qquad $\mathbf{F}(\mathbf{u}) = 0$
	\begin{itemize}
	\item[$\circ$] residual $\mathbf{F}:\RR^N \to \RR^N$ is generally nonlinear
	\item[$\circ$] $\mathbf{F}(\mathbf{w}) = \mathbf{b} - A \mathbf{w}$ in the linear case
	\end{itemize}

\bigskip\medskip
\noindent \textbf{definition.}  \begin{minipage}[t]{80mm}
an algorithm which solves $\mathbf{F}(\mathbf{u}) = 0$ in $O(N)$ work, as $N\to\infty$, is \alert{\emph{optimal}}
\end{minipage}

\bigskip \bigskip
\item if you have ever tried solving big, nontrivially-coupled systems of equations, you'll conclude optimality is generally hopeless
\end{itemize}
\end{frame}


\begin{frame}{slide full of caveats}

in the definition ``an algorithm which solves $\mathbf{F}(\mathbf{u}) = 0$ in $O(N)$ work, as $N\to\infty$, is \emph{optimal}'':
\begin{itemize}
\item  ``solves'' means: generates $\mathbf{u}_n$ so that $\frac{\|\mathbf{F}(\mathbf{u}_n)\|}{\|\mathbf{F}(\mathbf{u}_0)\|} \le \text{tol}$
	\begin{itemize}
	\item[$\circ$] where $\mathbf{u}_0$ is an initial guess
	\item[$\circ$] in linear case $A\bu=\bb$, given any rounding error, only $O(\kappa(A)\eps_{\text{mach}})$ accuracy is possible anyway\footnote{for students of MATH 614}
	\end{itemize}
\item ``$O(N)$'' hides constant; may depend on tol but not on $N$
\item ``work'' $=$ (count of floating point operations)
	\begin{itemize}
	\item[$\circ$] or runtime, but timing on modern computers is really messy
	\end{itemize}
\item ``$N\to\infty$'' limit is notional only
	\begin{itemize}
	\item[$\circ$] real computers run out of memory

	\smallskip
	\item[$\circ$] \alert{optimal algorithms are often memory-limited} \scriptsize \hfill (that's a \emph{feature})
	\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{example: tridiagonal matrices}
\begin{itemize}
\item an easy example of optimality
\item Gauss elimination solves $A \bu = \bb$ in $8N-6=O(N)$ flops
	\begin{itemize}
	\item[$\circ$] need to avoid pivoting
	\end{itemize}
\item for SPD tridiagonal matrices, use Cholesky decomposition, again $O(N)$
\item the 1D Poisson problem $-u''=f$, and generally all ODE BVPs, have optimal solution methods
\end{itemize}

\bigskip
\begin{center}
\includegraphics[width=0.2\textwidth]{figs/spytri}
\end{center}
\end{frame}


\begin{frame}{non-example:  banded direct methods in 2D,3D}
\begin{itemize}
\item for structured-grid FDM method on PDEs in 2D and 3D the bandwidth of $A$ grows as $N \to \infty$
\item for example, for MSE on $\Omega=[0,1]\times[0,1]$:

\begin{center}
\begin{tikzpicture}[scale=1.6,baseline] \input{tikz/unitsquaregridordering.tex} \end{tikzpicture} \qquad \begin{minipage}[b]{5mm} $\to$ \vspace{10mm} \end{minipage} \qquad \includegraphics[width=0.19\textwidth]{figs/spybanded}
\end{center}

\item if bandwidth $p$ then Cholesky does $O(N p^2)$ work
\item thus for direct methods for PDE problems on structured grids with $m$ points in each direction:
	\begin{itemize}
	\item[$\circ$] $N=m^2$ and $p=m$ so $O(N^2)$ work in 2D
	\item[$\circ$] $N=m^3$ and $p=m^2$ so $O(N^{7/3})$ work in 3D
	\end{itemize}
\item variable reordering helps a lot \dots but not enough
\end{itemize}
\end{frame}


\begin{frame}{sparse matrices from PDEs have $O(N)$ mat-vec}

\begin{itemize}
\item if
    \begin{itemize}
    \item[$\circ$] $A \in \RR^{N\times N}$ is sparse, with
    \item[$\circ$] number of nonzeros per row bounded independent of $N$
    \end{itemize}
then the \emph{work of computing $A \bv$ is $O(N)$}
\item computing $A \bv$ is called a ``mat-vec''
\item condition is automatic for structured grids and any \FM
\item for typical FEM on an unstructured mesh,
   $$\text{(nonzeros in row } j \text{ of } A) = \operatorname{degree}(\text{node } j) + 1$$
so cost of $A\bv$ is $O((\max \operatorname{degree}) N)$
\end{itemize}

\smallskip
\begin{center}
\includegraphics[width=0.65\textwidth]{figs/spythree}

\tiny

nonzeros per row $=$ $3$ \qquad\qquad\qquad $9$ \quad\qquad\qquad\qquad $\max \operatorname{degree} + 1$
\end{center}
\end{frame}


\begin{frame}{Krylov methods}
\begin{itemize}
\item among numerical analysts of the 1960s and 1970s, ``sparse mat-vecs are $O(N)$'' was the new hope
\item \begin{minipage}[t]{0.7\textwidth}
because naval engineer A.~Krylov (1931) observed that the solution to $A\bu=\bb$ may be well-approximated by $\bv$ in

\vspace{-4mm}
$$\mathcal{K}_m(A,\bb) = \operatorname{span}\left\{\bb,A\bb,A^2\bb,\dots,A^m\bb\right\}$$
\end{minipage} \quad
\begin{minipage}[t]{0.18\textwidth}
\vspace{-3mm}

\includegraphics[width=\textwidth]{figs/krylov}
\end{minipage}

    \vspace{2mm}
	\begin{itemize}
	\item[$\circ$] computing $\bv \in \mathcal{K}_m(A,\bb)$ costs $O(mN)$
	\item[$\circ$] $\bv \in \mathcal{K}_m(A,\bb) \iff \bv = p_m(A) \bb$
	\end{itemize}
\item to solve $A\bu=\bb$ we want $\bu = A^{-1}\bb \approx p_m(A) \bb$
\end{itemize}
\end{frame}


\begin{frame}{conjugate gradients}

\begin{itemize}
\item example: \emph{conjugate gradients} (CG) is a Krylov method
	\begin{itemize}
	\item[$\circ$] $A$ must be SPD because CG is a minimization algorithm
	\item[$\circ$] CG generates the ``best'' iterates $\bu_m$ from a Krylov space
		\begin{itemize}
		\item the error $\be_m=\bu_m-\bu$ is minimal in norm $\|\cdot\|_A$
		\end{itemize}
	\item[$\circ$] work per CG iteration is $O(N)$
	\item[$\circ$] thus work is $O(mN)$ for $m$ iterations
	\end{itemize}
\item if $\kappa=\kappa_2(A)$ is the condition number of $A$ then
	$$\frac{\|\be_m\|_A}{\|\be_0\|_A} \le 2 \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^m$$
\item it follows that $m = O(\sqrt{\kappa})$ iterations are needed
\end{itemize}
\end{frame}


\begin{frame}{CG iterations increase with $N$}

\begin{itemize}
\item unfortunately, if $A$ is from FDM applied to the Poisson equation then
    $$\kappa_2(A) = O(h^{-2})$$
\item \begin{minipage}[t]{55mm}
the number of CG iterations $m$ increases with $N=O(h^{-d})$
	\begin{itemize}
	\item[$\circ$] $m=O(N^{1/2})$ in 2D $\implies O(N^{3/2})$ solver
	\item[$\circ$] $m=O(N^{1/3})$ in 3D $\implies O(N^{4/3})$ solver
	\end{itemize}
\end{minipage} \quad
\begin{minipage}[t]{40mm}
\vspace{-2mm}

\includegraphics[width=\textwidth]{figs/poisson-cg-scale}
\end{minipage}

\vspace{-2mm}
\item other Krylov methods are similar

\vspace{5mm}
\item general reminder:

\begin{quote}
\alert{a Krylov method can only be optimal if the number of iterations $m$ is bounded independently of $N$}
\end{quote}
\end{itemize}
\end{frame}


\begin{frame}{preconditioning}
\begin{itemize}
\item by $\sim$1995 it was clear that Krylov methods by themselves were not the answer for solving big PDE problems
\item but you don't have to accept the given system $A\bu=\bb$
\item \emph{definition}. given invertible $M$,
	\begin{itemize}
	\item[$\circ$] $(M^{-1}A)\bu=M^{-1}\bb$ is the \emph{left-preconditioned} system
	\item[$\circ$] $(AM^{-1})M\bu=\bb$ is the \emph{right-preconditioned} system
	\end{itemize}
\item new condition numbers $\kappa_2(M^{-1}A)$ or $\kappa_2(AM^{-1})$ can be much smaller
\item ``$M^{-1}$'' must be a cheap method for this to help
	\begin{itemize}
	\item[$\circ$] e.g.~Meijerink \& van der Vorst (1977): incomplete LU and Cholesky factorizations
	\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{where we stand: an optimality lemma}


\begin{itemize}
\item \emph{lemma}.  as $N\to\infty$, if $A\in\RR^{N\times N}$ is SPD and if a symmetric preconditioning method produces bounded condition numbers,
     $$\kappa_2(M^{-1}A) \le B,$$
where $B>0$ is independent of $N$, then preconditioned CG is an optimal solver

\vspace{5mm}
\item \emph{optimality goal}:  find preconditioners which make $\kappa_2(M^{-1}A)$ bounded independent of $N$

\bigskip
\item \emph{multigrid} is such a preconditioner
\end{itemize}
\end{frame}


\section{multigrid}

\begin{frame}{multigrid: what it does}

\begin{itemize}
\item given sequence of grids $\{\Omega^{(j)}\}_0^k$ \quad \begin{tikzpicture}[scale=0.75,baseline]
\input{tikz/fourlevelsofgrids.tex}
\end{tikzpicture}
\item given initial guess $\bw$ on the finest grid $\Omega^{(k)}$, a multigrid cycle (e.g.~a ``V cycle'') approximately solves $A \bu = \bb$
\end{itemize}

\begin{minipage}[t]{75mm}
\begin{algorithmic}
\footnotesize
\Function{Vcycle}{$A,\bb,\bw,l$}
    \If{$l == 0$}
        \State solve $A \bv = \bb$, e.g.~by a direct solver
    \Else
        \State improve $\bw$ on the level $l$ grid
        \State $\br^C = \left(\text{restrict } \br = \bb - A \bw \text{ to } \Omega^{(l-1)}\right)$
        \State $\bz^C = \text{\textsc{Vcycle}}(A^C,\br^C,0,l-1$)
        \State $\bv \gets \bv + \left(\text{interpolate } \bz^C \text{ to } \Omega^{(l)}\right)$
        \State improve $\bv$ some more on the level $l$ grid
    \EndIf

    \noindent \quad\, \Return $\bv$
\EndFunction
\end{algorithmic}
\end{minipage}
\quad
\begin{minipage}[t]{25mm}
\vspace{0mm}

\input{tikz/vcycle.tex}
\end{minipage}
\end{frame}


\begin{frame}{multigrid uses cheap smoothers}

\begin{itemize}
\item \emph{question}: what does ``improve $\bw$ on the level $l$ grid'' mean?

\emph{answer}: \alert{smoothing}
\item many classical linear iterations are smoothing
	\begin{itemize}
	\item[$\circ$] e.g.~weighted Jacobi and Gauss-Seidel using $A$
	\item[$\circ$] fast $O(N)$ operations
	\item[$\circ$] single iteration reduces high-frequency components of the error
	\end{itemize}
\end{itemize}

\begin{center}
\includegraphics[width=0.25\textwidth]{figs/ps-unsmoothed}

\vspace{-3mm}

\begin{tikzpicture}[scale=2.0]
  \draw[-Stealth] (-0.8,0.0) -- (-1.2,-0.25) node [midway,rotate=30,above] {\tiny Jacobi};
  \draw[-Stealth] (0.0,0.0) -- (0.0,-0.25) node [midway,left] {\tiny weighted} node [midway,right] {\tiny Jacobi};
  \draw[-Stealth] (0.8,0.0) -- (1.2,-0.25) node [midway,rotate=-30,above] {\tiny Gauss-Seidel};
\end{tikzpicture}

\mbox{\includegraphics[width=0.2\textwidth]{figs/ps-jacobismoothed} \qquad
\includegraphics[width=0.2\textwidth]{figs/ps-wjacobismoothed} \qquad
\includegraphics[width=0.2\textwidth]{figs/ps-gssmoothed}}
\end{center}
\end{frame}


\begin{frame}{multigrid: why it is $O(N)$}

\begin{itemize}
\item multigrid is a systematic way of combining two actions
	\begin{itemize}
	\item[$\circ$] \emph{smoothing}: filter out high frequencies of the error on your grid
	\item[$\circ$] \emph{coarsening}: transfer to an easier grid
    	\begin{itemize}
	    \item frequencies that were ``medium-low'' are now ``high''
    	\end{itemize}
	\end{itemize}
\item restricting and interpolating on $\Omega^{(l)}$ is $O(N_l)$
\item a single step of the smoother on $\Omega^{(l)}$ is $O(N_l)$
\item thus total work on $\Omega^{(l)}$ is $C N_l$ for fixed $C$
\item then the total work of a V-cycle is a finite geometric series:

\vspace{-3mm}
\small
\begin{align*}
&(\Omega^{(k)} \text{ work}) + (\Omega^{(k-1)} \text{ work}) + \dots + (\Omega^{(1)} \text{ work}) + (\Omega^{(0)} \text{ work}) \\
&\quad = C N_k + C N_{k-1} + \dots + C N_1 + C_0 \\
&\quad = C N_k + C \frac{N_k}{2^d}  + \dots + C \left(\frac{1}{2^d}\right)^{k-1} N_k + C_0 \\
&\quad \le 2 C N_k + C_0
\end{align*}

    \vspace{-2mm}
	\begin{itemize}
	\item[$\circ$] re the coarsest grid \dots who cares! \dots $C_0$ is independent of $N$
	\end{itemize}

\end{itemize}
\end{frame}

\newcommand{\niceprob}{V-cycle-preconditioned CG iterations on $\Omega=[0,1]^d$ Poisson}
\newcommand{\niceprobtwo}{V-cycle-preconditioned CG iterations on $\Omega=[0,1]^2$ Poisson}

\begin{frame}{multigrid on Poisson}
\begin{itemize}
\item \niceprobtwo
\end{itemize}

\begin{center}
\includegraphics[width=0.6\textwidth]{figs/pccondition}

\small values of $\kappa_2(M^{-1}A)$ for 2D problem
\end{center}
\end{frame}


\begin{frame}{multigrid on Poisson: evidence of optimality}
\begin{itemize}
\item \niceprob
\end{itemize}

\begin{center}
\includegraphics[width=0.6\textwidth]{figs/optimal-flops}

\small direct demonstration of $O(N)$ work
\end{center}
\end{frame}


\begin{frame}{multigrid on Poisson: evidence of optimality}
\begin{itemize}
\item \niceprob
\end{itemize}

\begin{center}
\includegraphics[width=0.6\textwidth]{figs/optimal-flopsperN}

\small i.e.~constant amount of work per degree of freedom
\end{center}
\end{frame}


\begin{frame}{multigrid on Poisson: evidence of optimality}
\begin{itemize}
\item \niceprob
\end{itemize}

\begin{center}
\includegraphics[width=0.6\textwidth]{figs/optimal-timeperN}

\small (almost) constant amount of time per degree of freedom
\end{center}
\end{frame}


\begin{frame}{multigrid on MSE}
\begin{itemize}
\item recall the nonlinear MSE problem on $\Omega=[0,1]^2$:
\small
    $$- \grad\cdot \left(\frac{\grad u}{\sqrt{1 + |\grad u|^2}}\right) = 0  \qquad \text{ with } u\big|_{\partial \Omega} = g.$$
\normalsize
    \vspace{-3mm}
    \begin{itemize}
    \item[$\circ$] solved by Newton iteration
    \item[$\circ$] how to \emph{find a convergent initial iterate} on a fine grid?
    \end{itemize}
\item multigrid solution is by nonlinear ``F-cycle''
    \begin{itemize}
    \item[$\circ$] a.k.a.~nested iteration with V-cycles
    \item[$\circ$] \emph{start} on coarse grid $\Omega^{(0)}$
    \item[$\circ$] interpolating upward supplies good initial iterate
    \end{itemize}
\end{itemize}

\begin{center}
\begin{tikzpicture}[scale=1.0]
\input{tikz/fullcycle.tex}
\end{tikzpicture}
\end{center}
\end{frame}


\begin{frame}{multigrid on MSE: evidence of optimality}

\begin{center}
\includegraphics[width=0.6\textwidth]{figs/minoptimal}
\end{center}
\begin{itemize}
\item on finest $2049\times 2049$ grid with $N=4\times 10^6$:
    $$\text{total flops} = N \, \left(\frac{\text{flops}}{N}\right) = (4\times 10^6) (7 \times 10^3) \approx 3 \times 10^{10}$$
    \vspace{-3mm}
    \begin{itemize}
    \item[$\circ$] runtime about 5 minutes total
    \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{algebraic multigrid}
\begin{itemize}
\item what about multigrid on unstructured grids?
    \begin{itemize}
    \item[$\circ$] remember the snowflake?
    \item[$\circ$] for ``geometric'' multigrid (used so far) one need subgrids and grid-based restriction and interpolation operations
    \end{itemize}
\item new idea $\sim$1985 is \emph{algebraic multigrid}
    \begin{itemize}
    \item[$\circ$] can be applied to \emph{any} linear system $A\bu=\bb$
    \item[$\circ$] extracts analogs of ``subgrid'' and ``smoother'' and ``interpolation'' from $A$ itself
    \item[$\circ$] \dots but tends to need elliptic PDEish properties to actually work
    \item[$\circ$] active research area for e.g.~spectral graph theory
    \item<2>[$\circ$] by a new generation of applied mathematicians
    \end{itemize}
\end{itemize}

\begin{center}
\only<1>{\vspace{19mm}}
\includegraphics<2>[width=0.2\textwidth]{figs/urschel}
\end{center}
\end{frame}


\begin{frame}{algebraic multigrid: evidence of (near-)optimality}
\begin{itemize}
\item \begin{minipage}[t]{65mm}
recall unstructured mesh on a snowflake polygon; level 2 at right
\end{minipage} \quad \begin{minipage}[t]{20mm}
\begin{tikzpicture}[scale=0.7,baseline] \input{tikz/meshedsnowflake.tex} \end{tikzpicture}
\end{minipage}
\item generate level 5,6,7,8,9 approximations of the Koch fractal
\item mesh them
\item and test algebraic-multigrid-preconditioned CG method, on the Poisson equation $-\grad^2 u = 1$, for optimality
\end{itemize}

\begin{center}
\includegraphics[width=0.25\textwidth]{figs/snowflake} \qquad \includegraphics[width=0.55\textwidth]{figs/gamgopt}
\end{center}
\end{frame}


\begin{frame}{wider applicability of multigrid}

\begin{columns}
\begin{column}{0.8\textwidth}
\begin{itemize}
\item multigrid was invented for Poisson and linear elliptic equations by Federenko (1962, 1964)
\item but there was a period of darkness
\item by 1980 or so optimism about multigrid was limited to one mathematician: Achi Brandt
    \begin{itemize}
    \item[$\circ$] a creator of algebraic multigrid,
    \item[$\circ$] and of a fully-nonlinear multigrid, the \emph{full approximation scheme} (not covered here),
    \item[$\circ$] who started calling optimality ``textbook multigrid efficiency,'' which isn't really helping
    \end{itemize}

\bigskip
\item since then, multigrid has succeeded on more and more applications
    \begin{itemize}
    \footnotesize
    \item[$\circ$] example: Brown et al.~(2013), \emph{Achieving textbook multigrid efficiency for hydrostatic
ice flow}
    \end{itemize}
\end{itemize}
\end{column}

\begin{column}{0.2\textwidth}

\includegraphics[width=\textwidth]{figs/starwars-achi-mashup.jpg}

\tiny Achi Brandt is my hero

\vspace{10mm}
\includegraphics[width=\textwidth]{figs/jed.jpg}

\tiny Jed Brown, UAF MS 2006
\end{column}
\end{columns}
\end{frame}


\section{barriers \& extensions to optimality}

\begin{frame}{low regularity of solution}
\begin{itemize}
\item multigrid is not always easy to make optimal
\item 
\begin{minipage}[t]{60mm}
consider nonlinear PDE BVP
    \begin{itemize}
    \item[$\circ$] or a PDE system like Stokes flow
    \item[$\circ$] or an implicit time step of Navier-Stokes
    \end{itemize}
\end{minipage} \quad
\begin{minipage}[t]{35mm}
\vspace{0mm}
\includegraphics[width=1.1\textwidth]{figs/rayleigh-taylor-instability}
\end{minipage}

\vspace{-3mm}
\item try Newton-multigrid method
    \begin{itemize}
    \item[$\circ$] as we did with MSE
    \end{itemize}
\item \emph{problem}:  often \# of Newton steps \emph{and} Krylov steps grows as $h\to 0$ because of large gradients in the solution
\item \begin{minipage}[t]{75mm}
can demonstrate this with MSE for nonsmooth boundary conditions
    \vspace{-2mm}
    \begin{itemize}
    \item[$\circ$] not shown
    
    \vspace{-2mm}
    \item[$\circ$] Brandt suggests: combine multigrid \& AMR
    \end{itemize}
\end{minipage} \quad
\begin{minipage}[t]{20mm}
\vspace{0mm}
\begin{tikzpicture}[scale=1.1] \input{tikz/tent.tex} \end{tikzpicture}
\end{minipage}
\end{itemize}

\end{frame}


\begin{frame}{constrained problems}
\begin{itemize}
\item other problems are not quite PDEs because they have inequality constraints
\item causes two difficulties for Newton-multigrid methods:
    \begin{enumerate}
    \item a free boundary implies low regularity (last slide)
    \item \# of Newton steps is proportional to $D/h$ where $D$ is distance-to-move free boundary (from initial iterate)
    \end{enumerate}
\item but there's another Brandt invention: \emph{projected full approximation scheme}, a fully-nonlinear and constraint-adapted multigrid
\item Max H.~is working on it
\end{itemize}

\begin{center}
\includegraphics[width=0.37\textwidth]{figs/classicalobs}
\end{center}
\end{frame}


\begin{frame}{numerical convergence, and spectral methods}
\begin{itemize}
\item for a PDE BVP, as $h\to 0$ and $N\to\infty$ we \alert{first want convergence to the continuum solution}
    \begin{itemize}
    \item[$\circ$] I have been assuming that our methods are convergent!
    \end{itemize}
\item spectral methods get very close to the continuum solution for very small $N$ \hfill \scriptsize \dots compared to \FM \normalsize
    \begin{itemize}
    \item[$\circ$] \emph{if} the geometry is simple (rectangle)
    \item[$\circ$] \emph{and} the solution is smooth
    \end{itemize}
\item often $A$ is dense
\item but with such a spectral method, even $O(N^3)$ solution of the discrete equations is often acceptable because $N$ is small

\bigskip
\item recalling these things shows that optimality is \emph{not} the only good goal
\end{itemize}
\end{frame}


\begin{frame}{optimality, by numerical DE subfield}
\begin{itemize}
\item for all DE problems, one needs to define $N$, the number of discrete degrees of freedom, before we can talk optimality
\only<1>{
\item ODE IVP
    \begin{itemize}
    \item[$\circ$] for $y'=f(t,y)$ and $y(t) \in \RR^q$
    \item[$\circ$] on $[0,T]$ with $\Delta t$ spacing in time
    \item[$\circ$] define:
       $$N := \frac{T}{\Delta t}\,q$$
    \item[$\circ$] with this $N$, all ODE IVP methods are already optimal
    \end{itemize}
}
\only<2->{\item ODE IVP \hfill \alert{already optimal $\checkmark$}}
\only<2>{
\item ODE BVP
    \begin{itemize}
    \item[$\circ$] \FM generate tridiagonal systems
    \item[$\circ$] already optimal
    \end{itemize}
}
\only<3->{\item ODE BVP \hfill \alert{already optimal $\checkmark$}}
\only<3>{
\item \begin{minipage}[t]{60mm}
hyperbolic PDE IBVP
    \begin{itemize}
    \item[$\circ$] for example, wave equation with reaction: $u_t + a(u)\cdot \grad u = f(u)$
    \item[$\circ$] F$\begin{smallmatrix} \text{D} \\ \text{V} \end{smallmatrix}$Ms normally use explicit, CFL-limited time steps $\Delta t$
    \item[$\circ$] if we define $N = \frac{T}{\Delta t}\,\frac{L}{\Delta x}$, these methods are already optimal
    \end{itemize}
\end{minipage} \quad
\begin{minipage}[t]{35mm}
\vspace{0mm}
\begin{tikzpicture}[scale=2.0] \input{tikz/spacetimegrid.tex} \end{tikzpicture}
\end{minipage}
}
\only<4->{
\item hyperbolic PDE IBVP \hfill \alert{already optimal $\checkmark$}
\item PDE BVP \hfill \alert{optimal requires effort}
\item other PDE IBVP \hfill \alert{optimal requires effort}
    \begin{itemize}
    \item[$\circ$] for example, advection-diffusion-reaction equation with reaction: $u_t + a(u)\cdot \grad u = \Div(D\grad u) + f(u)$
    \end{itemize}
}
\end{itemize}
\end{frame}


\begin{frame}{conflicting goal: parallel scaling}
\begin{itemize}
\item sometimes you have a big machine with $\hat P$ processors 
    \begin{itemize}
    \item[$\circ$] everything so far has been serial ($P=1$)
    \item[$\circ$] on graph below, runtime is third axis
    \end{itemize}
\end{itemize}

\begin{center}
\only<1>{\hspace{0mm}}
\includegraphics<1>[width=0.65\textwidth]{figs/NPplane}
\includegraphics<2>[width=0.65\textwidth]{figs/NPplanestatic}
\includegraphics<3>[width=0.65\textwidth]{figs/NPplaneweakstrong}
\end{center}
\end{frame}


\begin{frame}{conclusion}
\begin{itemize}
\item in approximately solving your well-behaved PDE-type problem on a modern computer,

\bigskip
\begin{quote}
\alert{you should be solving the $N$ equations in $O(N)$ work}
\end{quote}

\bigskip
\noindent as you head toward $N=\infty$ unknowns
\item this is a good \emph{goal}\only<2>{\footnote{for dense systems $A\bu=\bb$ the goal is $O(N^2)$ \dots an open problem}} for PDE BVPs
\item to achieve it you must exploit
    \begin{itemize}
    \item[$\circ$] the locality (sparsity) of the problem, and
    \item[$\circ$] correlation (smoothness) of the solution
    \end{itemize}
\item multigrid is the only real hope!?
\item talk to me about PETSc \dots I am writing a book about that
\end{itemize}
\end{frame}


\end{document}
